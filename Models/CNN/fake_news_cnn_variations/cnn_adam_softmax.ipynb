{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmUcDEv25L5W"
      },
      "source": [
        "# Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LiyaZBkdqil",
        "outputId": "f2c3c792-793f-41da-f1dd-a49bf55427c5"
      },
      "outputs": [],
      "source": [
        "# Use this to install libraries if you find them missing on your system:\n",
        "#!pip install bs4\n",
        "#!pip install sklearn\n",
        "#!pip install nltk\n",
        "#!pip install gensim\n",
        "#!pip install lxml\n",
        "#!pip install keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uT92due2dsqS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, Flatten, Dropout, Concatenate\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
        "from keras.models import Model, model_from_json\n",
        "from keras.callbacks import EarlyStopping\n",
        "import gensim\n",
        "import gensim.downloader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score\n",
        "import tensorflow as tf\n",
        "import json\n",
        "import joblib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSdZSs0adFur",
        "outputId": "567790be-fc04-4c95-bb92-6843c446c77a"
      },
      "outputs": [],
      "source": [
        "word2vec = gensim.downloader.load('word2vec-google-news-300')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjJ43bOZnXpr"
      },
      "outputs": [],
      "source": [
        "EMBEDDING_DIM = 300\n",
        "MAX_VOCAB_SIZE = 262144\n",
        "MAX_SEQUENCE_LENGTH = 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKXJ0FNztvGv"
      },
      "outputs": [],
      "source": [
        "# Adapted from Yoon Kim model\n",
        "# Kim, Y. (2014). Convolutional neural networks for sentence classification. https://doi.org/10.48550/arXiv.1408.5882\n",
        "\n",
        "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index, trainable=False, extra_conv=True):\n",
        "\n",
        "    embedding_layer = Embedding(num_words,\n",
        "                            embedding_dim,\n",
        "                            weights=[embeddings],\n",
        "                            input_length=max_sequence_length,\n",
        "                            trainable=trainable)\n",
        "\n",
        "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
        "    embedded_sequences = embedding_layer(sequence_input)\n",
        "\n",
        "    convs = []\n",
        "    filter_sizes = [3,4,5]\n",
        "\n",
        "    for filter_size in filter_sizes:\n",
        "        l_conv = Conv1D(filters=128, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
        "        l_pool = MaxPooling1D(pool_size=3)(l_conv)\n",
        "        convs.append(l_pool)\n",
        "\n",
        "    l_merge = Concatenate(axis=-1)(convs)\n",
        "\n",
        "    conv = Conv1D(filters=128, kernel_size=3, activation='relu')(embedded_sequences)\n",
        "    pool = MaxPooling1D(pool_size=3)(conv)\n",
        "\n",
        "    if extra_conv==True:\n",
        "        x = Dropout(0.5)(l_merge)\n",
        "    else:\n",
        "        x = Dropout(0.5)(pool)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    preds = Dense(labels_index, activation='softmax')(x)\n",
        "\n",
        "    model = Model(sequence_input, preds)\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['acc'])\n",
        "    model.summary()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKQEZZarH3I2"
      },
      "source": [
        "# Part 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7JyWccIdsyq"
      },
      "outputs": [],
      "source": [
        "df_fake_1 = pd.DataFrame(pd.read_pickle(\"fake_news_data/clean/fake.pkl\"))\n",
        "df_real_1 = pd.DataFrame(pd.read_pickle(\"fake_news_data/clean/real.pkl\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ffNG_xBnrM3"
      },
      "outputs": [],
      "source": [
        "df_fake_1.columns = [\"text\"]\n",
        "df_real_1.columns = [\"text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tm-iY2Umg6EY"
      },
      "outputs": [],
      "source": [
        "df_real_1[\"label\"] = True\n",
        "df_fake_1[\"label\"] = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "FYvMiRhDds48",
        "outputId": "18bc791c-218d-4792-a3eb-9669f44c39b3"
      },
      "outputs": [],
      "source": [
        "df_1 = pd.concat([df_real_1,df_fake_1])\n",
        "df_1 = df_1.sample(frac = 1)\n",
        "df_1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kskuBZGpXnR"
      },
      "outputs": [],
      "source": [
        "X1 = df_1['text']\n",
        "y1 = df_1['label']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.1)\n",
        "X1_train, X1_test, y1_train, y1_test = X_train, X_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMpfzwF4sv5K",
        "outputId": "2e343847-a7c0-43a1-8e8e-a743446c96c5"
      },
      "outputs": [],
      "source": [
        "tokenizer1 = Tokenizer(num_words=MAX_VOCAB_SIZE,\n",
        "                      lower=True,\n",
        "                      char_level=False,\n",
        "                      filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
        "                      split=' ')\n",
        "\n",
        "tokenizer1.fit_on_texts(X1_train.tolist())\n",
        "\n",
        "train_word_index_1 = tokenizer1.word_index\n",
        "train_embedding_weights_1 = np.zeros((len(train_word_index_1)+1, EMBEDDING_DIM))\n",
        "\n",
        "for word, index in train_word_index_1.items():\n",
        "    if word in word2vec:\n",
        "        train_embedding_weights_1[index,:] = word2vec[word]\n",
        "    else:\n",
        "        train_embedding_weights_1[index,:] = np.random.rand(EMBEDDING_DIM)\n",
        "\n",
        "print(\"embedding dim:\", train_embedding_weights_1.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mKxhNuXteKv"
      },
      "outputs": [],
      "source": [
        "training_sequences_1 = tokenizer1.texts_to_sequences(X1_train.tolist())\n",
        "train_cnn_data_1 = pad_sequences(training_sequences_1, maxlen=MAX_SEQUENCE_LENGTH-1)\n",
        "\n",
        "test_sequences_1 = tokenizer1.texts_to_sequences(X1_test.tolist())\n",
        "test_cnn_data_1 = pad_sequences(test_sequences_1, maxlen=MAX_SEQUENCE_LENGTH-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7met_WVs65ca"
      },
      "outputs": [],
      "source": [
        "batch_size_1 = 256\n",
        "labels_1 = 1\n",
        "num_epochs_1 = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3X3hpXs3ukzQ",
        "outputId": "ee7a1ade-0080-412b-acbe-1905426a7848"
      },
      "outputs": [],
      "source": [
        "model1 = ConvNet(train_embedding_weights_1, MAX_SEQUENCE_LENGTH-1, len(train_word_index_1)+1, EMBEDDING_DIM, labels_1, False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NI1ZtG8Mwf_u"
      },
      "outputs": [],
      "source": [
        "tf.config.run_functions_eagerly(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQHMXftHvFe4",
        "outputId": "64067382-b73c-4b80-9bc6-b499834371b8"
      },
      "outputs": [],
      "source": [
        "hist = model1.fit(train_cnn_data_1, y1_train, epochs=num_epochs_1, validation_split=0.1, shuffle=True, batch_size=batch_size_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ST_1Ia3pw6dy",
        "outputId": "9795ca4c-241f-42a2-8ffb-a98614e38da1"
      },
      "outputs": [],
      "source": [
        "y1_predicted = model1.predict(test_cnn_data_1, batch_size=1024, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPWFCUhWw-q9"
      },
      "outputs": [],
      "source": [
        "y1_pred = np.round(y1_predicted)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "acc_model1 = round(accuracy_score(y1_test, y1_pred),4)\n",
        "prec_model1 = round(precision_score(y1_test, y1_pred, average=\"weighted\"),4)\n",
        "rec_model1 = round(recall_score(y1_test, y1_pred, average=\"weighted\"),4)\n",
        "f1_model1 = round(f1_score(y1_test, y1_pred, average=\"weighted\"),4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluation_dict = {\"accuracy:\": acc_model1 ,\"precision\": prec_model1 ,\"recall:\": rec_model1 ,\"f1:\": f1_model1 }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7Df0g7l5UM9"
      },
      "source": [
        "# Part 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7hFB_ci5XfZ"
      },
      "outputs": [],
      "source": [
        "df_fake_2 = pd.DataFrame(pd.read_csv(\"fake.csv\"))\n",
        "df_fake_2 = df_fake_2.drop([\"title\", \"uuid\", \"ord_in_thread\", \"author\", \"published\", \"language\", \"crawled\", \"site_url\",\t\"country\"\t,\"domain_rank\"\t,\"thread_title\",\t\"spam_score\",\t\"main_img_url\",\t\"replies_count\", \"participants_count\", \"likes\", \"comments\", \"shares\"], axis = 1)\n",
        "df_fake_2[\"text\"] = df_fake_2[\"text\"].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlnKC2rnFYmE"
      },
      "outputs": [],
      "source": [
        "df_fake_2 = df_fake_2.join(pd.get_dummies(df_fake_2['type']))\n",
        "df_fake_2 = df_fake_2.drop('type', axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "i63SMlRiP4Gw",
        "outputId": "e186a145-6d96-4864-f3ac-47db7e870924"
      },
      "outputs": [],
      "source": [
        "df_fake_bs = df_fake_2[df_fake_2.bs == True]\n",
        "df_fake_notbs = df_fake_2[df_fake_2.bs != True]\n",
        "print(\"Number of rows with label 'bs':\", len(df_fake_bs),\"\\nNumber of rows with other labels:\", len(df_fake_notbs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_fake_bs_sample = df_fake_bs.sample(frac= 0.05)\n",
        "df_fake_new = pd.concat([df_fake_bs_sample, df_fake_notbs])\n",
        "df_fake_new = df_fake_new.sample(frac=1)\n",
        "df_fake_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "io3SddrJHEie"
      },
      "outputs": [],
      "source": [
        "train, test = train_test_split(df_fake_new, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OqO00XZR_1s"
      },
      "outputs": [],
      "source": [
        "X2_train = train[\"text\"]\n",
        "X2_test = test[\"text\"]\n",
        "\n",
        "label_names = [\"bias\", \"bs\", \"conspiracy\", \"fake\", \"hate\", \"junksci\", \"satire\", \"state\"]\n",
        "y2_train = train[label_names].values\n",
        "y2_test = test[label_names].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Px_O9VdkJKAS",
        "outputId": "97a6711a-4083-48e6-d2c5-16e63aff03c1"
      },
      "outputs": [],
      "source": [
        "tokenizer2 = Tokenizer(num_words=MAX_VOCAB_SIZE,\n",
        "                      lower=True,\n",
        "                      char_level=False,\n",
        "                      filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
        "                      split=' ')\n",
        "\n",
        "tokenizer2.fit_on_texts(X2_train.tolist())\n",
        "\n",
        "train_word_index_2 = tokenizer2.word_index\n",
        "train_embedding_weights_2 = np.zeros((len(train_word_index_2)+1, EMBEDDING_DIM))\n",
        "\n",
        "for word,index in train_word_index_2.items():\n",
        "    if word in word2vec:\n",
        "        train_embedding_weights_2[index,:] = word2vec[word]\n",
        "    else:\n",
        "        train_embedding_weights_2[index,:] = np.random.rand(EMBEDDING_DIM)\n",
        "\n",
        "print(\"embedding dim:\", train_embedding_weights_2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JIzO_jDJ7oA"
      },
      "outputs": [],
      "source": [
        "training_sequences_2 = tokenizer2.texts_to_sequences(X2_train.tolist())\n",
        "train_cnn_data_2 = pad_sequences(training_sequences_2, maxlen=MAX_SEQUENCE_LENGTH-1)\n",
        "\n",
        "test_sequences_2 = tokenizer2.texts_to_sequences(X2_test.tolist())\n",
        "test_cnn_data_2 = pad_sequences(test_sequences_2, maxlen=MAX_SEQUENCE_LENGTH-1)\n",
        "\n",
        "print(len(train_cnn_data_2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VI8UY6RJ_vt"
      },
      "outputs": [],
      "source": [
        "batch_size_2 = 256\n",
        "labels_2 = 8\n",
        "num_epochs_2 = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foe6rPSYKGVw",
        "outputId": "e9e0e13a-f17d-44bb-a0c1-f02a86861d27"
      },
      "outputs": [],
      "source": [
        "model2 = ConvNet(train_embedding_weights_2, MAX_SEQUENCE_LENGTH-1, len(train_word_index_2)+1, EMBEDDING_DIM, labels_2, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlUIWg34KKCk"
      },
      "outputs": [],
      "source": [
        "tf.config.run_functions_eagerly(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKpPY__BREWE",
        "outputId": "ff9aec45-8c8e-4c51-dba6-37e9bc626c46"
      },
      "outputs": [],
      "source": [
        "hist = model2.fit(train_cnn_data_2, y2_train, epochs=num_epochs_2, validation_split=0.1, shuffle=True, batch_size=batch_size_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhkU1Rt2SlC3",
        "outputId": "44745389-7e3c-499a-c121-b306fab30579"
      },
      "outputs": [],
      "source": [
        "y2_predicted = model2.predict(test_cnn_data_2, batch_size=1024, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Il8542xLTdy1"
      },
      "outputs": [],
      "source": [
        "df2_pred = pd.DataFrame(y2_predicted)\n",
        "df2_pred = df2_pred.where(df2_pred!=0).rank(1, ascending=False, method='dense').eq(1).astype(int)\n",
        "np2_pred = df2_pred.to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddixyUpETw16",
        "outputId": "67f1e7d4-d5e6-4ac3-f507-d81d42d37be0"
      },
      "outputs": [],
      "source": [
        "acc_model2 = round(accuracy_score(y2_test, np2_pred),4)\n",
        "prec_model2 = round(precision_score(y2_test, np2_pred, average=\"weighted\"),4)\n",
        "rec_model2 = round(recall_score(y2_test, np2_pred, average=\"weighted\"),4)\n",
        "f1_model2 = round(f1_score(y2_test, np2_pred, average=\"weighted\"),4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluation_dict = {\"accuracy:\": acc_model2 ,\"precision\": prec_model2 ,\"recall:\": rec_model2 ,\"f1:\": f1_model2 }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "to save model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "joblib.dump(model2, \"model2.sav\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_json = model2.to_json()\n",
        "model2.save_weights('model2_weights')\n",
        "with open('model2.json', 'w') as f:\n",
        "    json.dump(model_json, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.DataFrame(np2_pred).sample(frac= 0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Check Models Compatibility\n",
        "(using the model from Part 2 on the fake news obtained from Part 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_combine = pd.DataFrame(X1_test)\n",
        "df_combine[\"label\"] = y1_pred\n",
        "df_combine = df_combine.reset_index(drop=True)\n",
        "\n",
        "df_combine_fake = df_combine.loc[df_combine['label'] == 0.0]\n",
        "\n",
        "df_sample_fake = df_combine_fake.sample(frac=0.1)\n",
        "df_sample_fake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "combined_sample_tokenised = tokenizer2.texts_to_sequences(df_sample_fake[\"text\"].tolist())\n",
        "combined_sample_train_data = pad_sequences(combined_sample_tokenised, maxlen=MAX_SEQUENCE_LENGTH-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "combined_predicted = model2.predict(combined_sample_train_data, batch_size=1024, verbose=1)\n",
        "\n",
        "df_combined_pred = pd.DataFrame(combined_predicted)\n",
        "df_combined_pred = df_combined_pred.where(df_combined_pred!=0).rank(1, ascending=False, method='dense').eq(1).astype(int)\n",
        "\n",
        "df_combined_pred.columns = label_names \n",
        "df_combined_pred[\"text\"] = df_sample_fake[\"text\"].values\n",
        "df_combined_pred\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test both models on input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_news = input(\"Insert News\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_tokenised_1 = tokenizer1.texts_to_sequences([[text_news]])\n",
        "text_tokenised_1_data = pad_sequences(text_tokenised_1, maxlen=MAX_SEQUENCE_LENGTH-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_1_predicted = model1.predict(text_tokenised_1_data, batch_size=1024, verbose=1)\n",
        "text_1_predicted = np.round(np.array(text_1_predicted))\n",
        "text_1_predicted = np.bool8(text_1_predicted)\n",
        "text_1_predicted[0,0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_text2_pred = pd.DataFrame(None)\n",
        "\n",
        "if text_1_predicted[0,0] == 0:\n",
        "    text_tokenised_2 = tokenizer2.texts_to_sequences([[text_news]])\n",
        "    text_tokenised_2_data = pad_sequences(text_tokenised_2, maxlen=MAX_SEQUENCE_LENGTH-1)\n",
        "    text_2_predicted = model2.predict(text_tokenised_2_data, batch_size=1024, verbose=1)\n",
        "    df_text2_pred = pd.DataFrame(text_2_predicted)\n",
        "    df_text2_pred = df_text2_pred.where(df_text2_pred!=0).rank(1, ascending=False, method='dense').eq(1).astype(int)\n",
        "    df_text2_pred.columns = label_names \n",
        "    \n",
        "print(df_text2_pred)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
